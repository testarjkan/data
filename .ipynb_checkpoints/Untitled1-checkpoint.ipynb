{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Flights(date: String, month: String, carrier: String, flightnum: String, origin: String, dest: String, arrdelay: Double, depdelay: Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(x: String): Flights = {\n",
    "    val fields = x.split(\",\")\n",
    "    Flights(x(0).toString, x(1).toString, x(2).toString, x(3).toString, x(4).toString, x(5).toString, x(6).toDouble, x(7).toDouble)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.mapred.InvalidInputException\n",
       "Message: Input path does not exist: file:/Users/urjun/spark/data/spark/data/flights2.csv\n",
       "StackTrace: org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n",
       "org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n",
       "org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n",
       "org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n",
       "org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n",
       "scala.Option.getOrElse(Option.scala:120)\n",
       "org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n",
       "org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1293)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.take(RDD.scala:1288)\n",
       "$line15.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:24)\n",
       "$line15.$read$$iwC$$iwC$$iwC.<init>(<console>:29)\n",
       "$line15.$read$$iwC$$iwC.<init>(<console>:31)\n",
       "$line15.$read$$iwC.<init>(<console>:33)\n",
       "$line15.$read.<init>(<console>:35)\n",
       "$line15.$read$.<init>(<console>:39)\n",
       "$line15.$read$.<clinit>(<console>)\n",
       "$line15.$eval$.<init>(<console>:7)\n",
       "$line15.$eval$.<clinit>(<console>)\n",
       "$line15.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flrdd2 = sc.textFile(\"flights2.csv\").map(parse).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
